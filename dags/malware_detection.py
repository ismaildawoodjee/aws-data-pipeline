"""Airflow DAG for the AWS batch processing data pipeline"""

import os
import json
import utils
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.providers.amazon.aws.sensors.emr_job_flow import EmrJobFlowSensor
from airflow.providers.amazon.aws.sensors.redshift import AwsRedshiftClusterSensor
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator

from datetime import datetime, timedelta

# Get exported variables from Airflow
BUCKET_NAME = Variable.get("BUCKET_NAME")
REDSHIFT_CLUSTER_NAME = Variable.get("REDSHIFT_CLUSTER_NAME")

# Provide JOB_FLOW_OVERRIDES argument (JSON for EMR cluster creation)
with open(os.path.join(os.getcwd(), "dags/scripts/emr/create_emr.json"), "r") as f:
    JOB_FLOW_OVERRIDES = json.load(f)

# Specify arguments for Spark script - which script to read, where to read data from,
# and where to write data to. These are the sys.argv arguments in the Spark script
today = datetime.utcnow().date()
yesterday = today - timedelta(days=1)
S3_URI_SPARK_SCRIPT = "s3://malware-detection-bucket/scripts/spark/malware_file_detection.py"
S3_URI_RAW_FILE = f"s3://malware-detection-bucket/raw/malware_file_detection/{yesterday}/malware_files.csv"
S3_URI_STAGE = f"s3://malware-detection-bucket/stage/malware_file_detection/{yesterday}/"

JOB_FLOW_OVERRIDES["Steps"][0]["HadoopJarStep"]["Args"] += [
    S3_URI_SPARK_SCRIPT,
    S3_URI_RAW_FILE,
    S3_URI_STAGE,
]

# Default arguments for defining the DAG
# start_date: if today is 8th, and I want to run my daily DAG on 9th, specify 8th
# actual_start_date = start_date + schedule_interval
default_args = {
    "owner": "airflow",
    "start_date": datetime(2022, 1, 20),
    "depends_on_past": False,
    "email": ["airflow@airflow.com"],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=30),
}

# Running DAG everyday at midnight UTC
with DAG(
    dag_id="malware_detection",
    default_args=default_args,
    schedule_interval="0 0 * * *",
) as dag:

    # On Mondays, attach randomly generated timestamps to each row of the file
    attach_timestamp_to_file = PythonOperator(
        task_id="attach_timestamp_to_csv_MALWARE_FILE",
        python_callable=utils._attach_datetime,
        op_kwargs={
            "filename": "/source-data/malware_detection.csv",
            "destination": "/source-data/dated_malware_detection.csv",
        },
    )

    # copies postgres-source data into CSV file
    extract_malware_files_data = PostgresOperator(
        task_id="extract_sql_MALWARE_FILE_to_csv",
        sql="./scripts/sql/extract_malware_files.sql",
        postgres_conn_id="postgres_source",
        params={"malware_files": "/source-data/temp/malware_files.csv"},
    )

    # moves flat file from local system to S3 `raw` folder
    load_malware_files_to_s3_raw = PythonOperator(
        task_id="load_MALWARE_FILE_to_S3_RAW",
        python_callable=utils._local_file_to_s3,
        op_kwargs={
            "filename": "/source-data/temp/malware_files.csv",
            "key": f"raw/malware_file_detection/{yesterday}/malware_files.csv",
            "bucket_name": BUCKET_NAME,
            "remove_local": False,
        },
    )

    # uploads Python code to S3 `scripts` folder
    upload_code_to_s3_scripts = PythonOperator(
        task_id="upload_code_to_S3_SCRIPTS",
        python_callable=utils._local_file_to_s3,
        op_kwargs={
            "filename": "./dags/scripts/spark/malware_file_detection.py",
            "key": "scripts/spark/malware_file_detection.py",
            "bucket_name": BUCKET_NAME,
            "remove_local": False,
        },
    )

    # creates an EMR cluster according to the settings in JOB_FLOW_OVERRIDES
    # and then runs the specified Spark step
    # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html#EMR.Client.run_job_flow
    create_and_run_emr_cluster = EmrCreateJobFlowOperator(
        task_id="create_and_run_EMR_CLUSTER",
        job_flow_overrides=JOB_FLOW_OVERRIDES,
        aws_conn_id="aws_default",
        emr_conn_id="emr_default",
    )

    # waits for EMR step to be completed by using a Sensor
    check_emr_step_completion = EmrJobFlowSensor(
        task_id="check_EMR_STEP_completion",
        job_flow_id=create_and_run_emr_cluster.output,
    )

    # resume the Redshift cluster from its dormant state
    # what if Redshift is unavailable? better to have multiple nodes for data replication
    # https://stackoverflow.com/questions/70075639/aws-redshift-node-failure-is-the-entire-cluster-unavailable-despite-having-mut
    resume_redshift_cluster = PythonOperator(
        task_id="resume_REDSHIFT_CLUSTER",
        python_callable=utils._resume_redshift_cluster,
        op_kwargs={"cluster_identifier": REDSHIFT_CLUSTER_NAME},
    )

    # waits for Redshift to become available
    check_redshift_availability = AwsRedshiftClusterSensor(
        task_id="check_REDSHIFT_CLUSTER_availability",
        cluster_identifier=REDSHIFT_CLUSTER_NAME,
        target_status="available",
    )

    # operation to load processed data in `stage` into Redshift DWH
    load_malware_file_to_redshift = S3ToRedshiftOperator(
        task_id="load_csv_MALWARE_FILE_to_REDSHIFT",
        schema="public",
        table="malware_file",
        s3_bucket=BUCKET_NAME,
        s3_key=f"stage/malware_file_detection/{yesterday}/",
        copy_options=["csv"],
        redshift_conn_id="redshift_conn_id",
        aws_conn_id="aws_default",
        method="UPSERT",
        upsert_keys=["time_received"],
    )

    # load Redshift data to EC2's Postgres database
    load_supersetdb_from_redshift = PostgresOperator(
        task_id="load_SUPERSETDB_from_REDSHIFT",
        sql="./scripts/sql/from_redshift_to_supersetdb.sql",
        postgres_conn_id="supersetdb",
    )

    # pause the Redshift cluster to stop incurring compute costs
    pause_redshift_cluster = PythonOperator(
        task_id="pause_REDSHIFT_CLUSTER",
        python_callable=utils._pause_redshift_cluster,
        op_kwargs={"cluster_identifier": REDSHIFT_CLUSTER_NAME},
        trigger_rule="all_done",
    )

    # the complete DAG
    (
        attach_timestamp_to_file
        >> extract_malware_files_data
        >> [load_malware_files_to_s3_raw, upload_code_to_s3_scripts]
        >> DummyOperator(task_id="O")
        >> [resume_redshift_cluster, create_and_run_emr_cluster]
    )
    create_and_run_emr_cluster >> check_emr_step_completion
    (
        [resume_redshift_cluster, check_emr_step_completion]
        >> check_redshift_availability
        >> load_malware_file_to_redshift
        >> load_supersetdb_from_redshift
        >> pause_redshift_cluster
        >> DummyOperator(task_id="END")
    )
